{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Разбор_ДЗ_1  \"Июльский_курс_2020_Pro\"",
      "provenance": [],
      "collapsed_sections": [
        "R6iJpPIjHG_a",
        "Qh-pPGWkOp1o"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CCqcXtmcifQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist #Библиотека с базой Mnist\n",
        "from tensorflow.keras.models import Sequential # Подлючаем класс создания модели Sequential\n",
        "from tensorflow.keras.layers import Dense # Подключаем класс Dense - полносвязный слой\n",
        "from tensorflow.keras.optimizers import Adam # Подключаем оптимизатор Adam\n",
        "from tensorflow.keras import utils #Утилиты для to_categorical\n",
        "from tensorflow.keras.preprocessing import image #Для отрисовки изображения\n",
        "import numpy as np # Подключаем библиотеку numpy\n",
        "import pandas as pd # Подключаем библиотеку Pandas\n",
        "import pylab # Модуль для построения графиков\n",
        "from mpl_toolkits.mplot3d import Axes3D # Модуль для трехмерной графики\n",
        "from google.colab import files #Для загрузки своей картинки\n",
        "import matplotlib.pyplot as plt #Отрисовка изображений\n",
        "from PIL import Image #Отрисовка изображений\n",
        "#Отрисовывать изображения в ноутбуке, а не в консоль или файл\n",
        "%matplotlib inline "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6iJpPIjHG_a",
        "colab_type": "text"
      },
      "source": [
        "# Light\n",
        "\n",
        "Вариант 2\n",
        "\n",
        "Проведите серию экспериментов по перебору гиперпараметров нейронной сети, созданной на занятии.\n",
        "Поменяйте количество нейронов в сети, используя следующие значения:\n",
        "\n",
        "\n",
        "* Один слой 10 нейронов\n",
        "* Один слой 100 нейронов\n",
        "* Один слой 5000 нейронов\n",
        "\n",
        "\n",
        "Поменяйте активационную функцию в скрытых слоях с relu на linear.\n",
        "\n",
        "\n",
        "Поменяйте размеры batch_size:\n",
        "\n",
        "\n",
        "* 1\n",
        "* 10\n",
        "* 100\n",
        "* Вся база (60000)\n",
        "\n",
        "\n",
        "Запишите в таблицу получившиеся точности.\n",
        "Напишите выводы по результатам проведенных тестов.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asPzZnmLHcDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30e653ab-3f9b-47f4-a02c-627d023d739a"
      },
      "source": [
        "# Загрузка данных\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = mnist.load_data()\n",
        "\n",
        "# Меняем формат входных картинок с 28х28 на 784х1\n",
        "x_train = x_train_org.reshape(60000, 784)\n",
        "x_test = x_test_org.reshape(10000, 784)\n",
        "\n",
        "# Нормализуем данные\n",
        "x_train = x_train.astype('float32') # преобразовываем x_train в тип float (цифры с плавающей точкой)\n",
        "x_train = x_train / 255 # делим на 255, чтобы диапазон был от 0 до 1\n",
        "x_test = x_test.astype('float32') # преобразовываем x_test в тип float (цифры с плавающей точкой)\n",
        "x_test = x_test / 255 # делим на 255, чтобы диапазон был от 0 до 1\n",
        "\n",
        "# Преобразуем ответы в формат one_hot_encoding\n",
        "y_train = utils.to_categorical(y_train_org, 10)\n",
        "y_test = utils.to_categorical(y_test_org, 10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GihYe4o4eFCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aed514ce-d3aa-4ad9-a311-6a708fc9300f"
      },
      "source": [
        "# Проверяем размеры\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "(10000, 784)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae-GAspleDvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7d091d12-aa37-4cf3-bbc8-ad65a7a8e3c9"
      },
      "source": [
        "# Модель\n",
        "model_sample = Sequential() # Создаём сеть прямого распространения\n",
        "model_sample.add(Dense(800, input_dim=784, activation='relu')) # Добавляем полносвязный слой на 800 нейронов с relu-активацией\n",
        "model_sample.add(Dense(10, activation='softmax')) # Добавляем полносвязный слой на 10 нейронов с softmax-активацией\n",
        "\n",
        "model_sample.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Компилируем модель\n",
        "\n",
        "# Обучение сети\n",
        "\n",
        "# fit - функция обучения нейронки\n",
        "# x_train, y_train - обучающая выборка, входные и выходные данные\n",
        "# batch_size - размер батча, количество примеров, которое обрабатывает нейронка перед одним изменением весов\n",
        "# epochs - количество эпох, когда нейронка обучается на всех примерах выборки\n",
        "# verbose - 0 - не визуализировать ход обучения, 1 - визуализировать\n",
        "model_sample.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2797 - accuracy: 0.9196 - val_loss: 0.1454 - val_accuracy: 0.9585\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1105 - accuracy: 0.9678 - val_loss: 0.1078 - val_accuracy: 0.9668\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0698 - accuracy: 0.9796 - val_loss: 0.0938 - val_accuracy: 0.9720\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9860 - val_loss: 0.0929 - val_accuracy: 0.9713\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0347 - accuracy: 0.9898 - val_loss: 0.0761 - val_accuracy: 0.9761\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9935 - val_loss: 0.0759 - val_accuracy: 0.9782\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9958 - val_loss: 0.0770 - val_accuracy: 0.9778\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0862 - val_accuracy: 0.9757\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.0741 - val_accuracy: 0.9796\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.0789 - val_accuracy: 0.9796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdf20110978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJxr5OA4HBoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d9491ede-5583-4519-a445-42a45a584e2c"
      },
      "source": [
        "model_sample.evaluate(x_test, y_test)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98089998960495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-zEikOxhz_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdc0da84-d4a2-4cc9-8924-391c2c537431"
      },
      "source": [
        "data = [[800, 'relu', 128, round(model_sample.evaluate(x_test, y_test, verbose = 0)[1], 3)]]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[800, 'relu', 128, 0.981]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynpTysg9jR9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ed5f825-0729-4882-a982-0ccd2cee04f8"
      },
      "source": [
        "# 1\n",
        "N = [10, 100, 5000] # Кол-во нейронов в скрытом слое\n",
        "for i in N:\n",
        "    model = Sequential() \n",
        "    model.add(Dense(i, input_dim=784, activation='relu')) # Добавляем полносвязный слой на N нейронов\n",
        "    model.add(Dense(10, activation='softmax')) \n",
        "\n",
        "    # Компилируем модель\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "\n",
        "    # Обучение сети\n",
        "    model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)\n",
        "\n",
        "    data = data + [[i, 'relu', 128, round(model.evaluate(x_test, y_test, verbose = 0)[1], 3)]]\n",
        "\n",
        "\n",
        "# 2\n",
        "model = Sequential() \n",
        "model.add(Dense(800, input_dim=784, activation='linear')) # Меняем активационную функцию на 'linear'\n",
        "model.add(Dense(10, activation='softmax')) \n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "\n",
        "# Обучение сети\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)\n",
        "\n",
        "data = data + [[800, 'linear', 128, round(model.evaluate(x_test, y_test, verbose = 0)[1], 3)]]\n",
        "\n",
        "\n",
        "# 3\n",
        "N_batch_size = [1, 10, 100, 60000]\n",
        "for i in N_batch_size:\n",
        "    model = Sequential() # Создаём сеть прямого распространения\n",
        "    model.add(Dense(800, input_dim=784, activation='relu')) \n",
        "    model.add(Dense(10, activation='softmax')) \n",
        "\n",
        "    # Компилируем модель\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "\n",
        "    # Обучение сети\n",
        "    model.fit(x_train, y_train, batch_size=i, epochs=10, verbose=1, validation_split=0.2)  # Меняем batch_size\n",
        "\n",
        "    data = data + [[800, 'relu', i, round(model.evaluate(x_test, y_test, verbose = 0)[1], 3)]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.8836 - accuracy: 0.7392 - val_loss: 0.3902 - val_accuracy: 0.8949\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3623 - accuracy: 0.8992 - val_loss: 0.3014 - val_accuracy: 0.9170\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3087 - accuracy: 0.9124 - val_loss: 0.2745 - val_accuracy: 0.9239\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2856 - accuracy: 0.9195 - val_loss: 0.2649 - val_accuracy: 0.9263\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2713 - accuracy: 0.9234 - val_loss: 0.2556 - val_accuracy: 0.9288\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2615 - accuracy: 0.9265 - val_loss: 0.2502 - val_accuracy: 0.9301\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2528 - accuracy: 0.9289 - val_loss: 0.2444 - val_accuracy: 0.9315\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2464 - accuracy: 0.9304 - val_loss: 0.2401 - val_accuracy: 0.9331\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2404 - accuracy: 0.9324 - val_loss: 0.2396 - val_accuracy: 0.9320\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2358 - accuracy: 0.9331 - val_loss: 0.2358 - val_accuracy: 0.9337\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.4143 - accuracy: 0.8855 - val_loss: 0.2223 - val_accuracy: 0.9391\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1991 - accuracy: 0.9430 - val_loss: 0.1718 - val_accuracy: 0.9532\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1486 - accuracy: 0.9575 - val_loss: 0.1437 - val_accuracy: 0.9589\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1166 - accuracy: 0.9668 - val_loss: 0.1243 - val_accuracy: 0.9657\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0955 - accuracy: 0.9726 - val_loss: 0.1121 - val_accuracy: 0.9672\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0807 - accuracy: 0.9780 - val_loss: 0.1045 - val_accuracy: 0.9690\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0689 - accuracy: 0.9806 - val_loss: 0.1040 - val_accuracy: 0.9693\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0940 - val_accuracy: 0.9719\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 0.0897 - val_accuracy: 0.9723\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0438 - accuracy: 0.9880 - val_loss: 0.0912 - val_accuracy: 0.9721\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2128 - accuracy: 0.9357 - val_loss: 0.1006 - val_accuracy: 0.9693\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0796 - accuracy: 0.9760 - val_loss: 0.0886 - val_accuracy: 0.9713\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0494 - accuracy: 0.9843 - val_loss: 0.0766 - val_accuracy: 0.9770\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.0867 - val_accuracy: 0.9743\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.0724 - val_accuracy: 0.9796\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.0810 - val_accuracy: 0.9789\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.0971 - val_accuracy: 0.9748\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.0866 - val_accuracy: 0.9793\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.1086 - val_accuracy: 0.9767\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0868 - val_accuracy: 0.9811\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3768 - accuracy: 0.8920 - val_loss: 0.3003 - val_accuracy: 0.9171\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3079 - accuracy: 0.9133 - val_loss: 0.2884 - val_accuracy: 0.9217\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2939 - accuracy: 0.9170 - val_loss: 0.2957 - val_accuracy: 0.9181\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2894 - accuracy: 0.9200 - val_loss: 0.2784 - val_accuracy: 0.9251\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2836 - accuracy: 0.9200 - val_loss: 0.2951 - val_accuracy: 0.9198\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2785 - accuracy: 0.9215 - val_loss: 0.2833 - val_accuracy: 0.9223\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2762 - accuracy: 0.9226 - val_loss: 0.2886 - val_accuracy: 0.9213\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2737 - accuracy: 0.9224 - val_loss: 0.2916 - val_accuracy: 0.9226\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2704 - accuracy: 0.9241 - val_loss: 0.2920 - val_accuracy: 0.9224\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2684 - accuracy: 0.9247 - val_loss: 0.2889 - val_accuracy: 0.9200\n",
            "Epoch 1/10\n",
            "48000/48000 [==============================] - 100s 2ms/step - loss: 0.2494 - accuracy: 0.9320 - val_loss: 0.1690 - val_accuracy: 0.9622\n",
            "Epoch 2/10\n",
            "48000/48000 [==============================] - 99s 2ms/step - loss: 0.1644 - accuracy: 0.9647 - val_loss: 0.1906 - val_accuracy: 0.9660\n",
            "Epoch 3/10\n",
            "48000/48000 [==============================] - 99s 2ms/step - loss: 0.1464 - accuracy: 0.9726 - val_loss: 0.2449 - val_accuracy: 0.9595\n",
            "Epoch 4/10\n",
            "48000/48000 [==============================] - 101s 2ms/step - loss: 0.1223 - accuracy: 0.9784 - val_loss: 0.2645 - val_accuracy: 0.9646\n",
            "Epoch 5/10\n",
            "48000/48000 [==============================] - 99s 2ms/step - loss: 0.1216 - accuracy: 0.9814 - val_loss: 0.3280 - val_accuracy: 0.9697\n",
            "Epoch 6/10\n",
            "48000/48000 [==============================] - 100s 2ms/step - loss: 0.1113 - accuracy: 0.9833 - val_loss: 0.3385 - val_accuracy: 0.9707\n",
            "Epoch 7/10\n",
            "48000/48000 [==============================] - 102s 2ms/step - loss: 0.1074 - accuracy: 0.9854 - val_loss: 0.3338 - val_accuracy: 0.9731\n",
            "Epoch 8/10\n",
            "48000/48000 [==============================] - 100s 2ms/step - loss: 0.1011 - accuracy: 0.9869 - val_loss: 0.3727 - val_accuracy: 0.9712\n",
            "Epoch 9/10\n",
            "48000/48000 [==============================] - 99s 2ms/step - loss: 0.0845 - accuracy: 0.9885 - val_loss: 0.4548 - val_accuracy: 0.9697\n",
            "Epoch 10/10\n",
            "48000/48000 [==============================] - 100s 2ms/step - loss: 0.0733 - accuracy: 0.9904 - val_loss: 0.4522 - val_accuracy: 0.9723\n",
            "Epoch 1/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.1929 - accuracy: 0.9410 - val_loss: 0.1111 - val_accuracy: 0.9655\n",
            "Epoch 2/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0853 - accuracy: 0.9739 - val_loss: 0.1002 - val_accuracy: 0.9724\n",
            "Epoch 3/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0590 - accuracy: 0.9814 - val_loss: 0.0957 - val_accuracy: 0.9735\n",
            "Epoch 4/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0460 - accuracy: 0.9850 - val_loss: 0.0914 - val_accuracy: 0.9763\n",
            "Epoch 5/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0349 - accuracy: 0.9889 - val_loss: 0.1092 - val_accuracy: 0.9745\n",
            "Epoch 6/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0296 - accuracy: 0.9912 - val_loss: 0.1103 - val_accuracy: 0.9776\n",
            "Epoch 7/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.1122 - val_accuracy: 0.9779\n",
            "Epoch 8/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0247 - accuracy: 0.9925 - val_loss: 0.1384 - val_accuracy: 0.9754\n",
            "Epoch 9/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0211 - accuracy: 0.9937 - val_loss: 0.1268 - val_accuracy: 0.9771\n",
            "Epoch 10/10\n",
            "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0198 - accuracy: 0.9948 - val_loss: 0.1446 - val_accuracy: 0.9753\n",
            "Epoch 1/10\n",
            "480/480 [==============================] - 1s 3ms/step - loss: 0.2583 - accuracy: 0.9253 - val_loss: 0.1418 - val_accuracy: 0.9584\n",
            "Epoch 2/10\n",
            "480/480 [==============================] - 1s 2ms/step - loss: 0.1016 - accuracy: 0.9706 - val_loss: 0.1018 - val_accuracy: 0.9695\n",
            "Epoch 3/10\n",
            "480/480 [==============================] - 1s 2ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.0830 - val_accuracy: 0.9747\n",
            "Epoch 4/10\n",
            "480/480 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9868 - val_loss: 0.0876 - val_accuracy: 0.9743\n",
            "Epoch 5/10\n",
            "480/480 [==============================] - 1s 2ms/step - loss: 0.0293 - accuracy: 0.9919 - val_loss: 0.0722 - val_accuracy: 0.9782\n",
            "Epoch 6/10\n",
            "480/480 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9937 - val_loss: 0.0988 - val_accuracy: 0.9710\n",
            "Epoch 7/10\n",
            "480/480 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9956 - val_loss: 0.0888 - val_accuracy: 0.9759\n",
            "Epoch 8/10\n",
            "480/480 [==============================] - 1s 3ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.0768 - val_accuracy: 0.9793\n",
            "Epoch 9/10\n",
            "480/480 [==============================] - 1s 3ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.0776 - val_accuracy: 0.9809\n",
            "Epoch 10/10\n",
            "480/480 [==============================] - 1s 3ms/step - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.0884 - val_accuracy: 0.9772\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 2.3245 - accuracy: 0.0975 - val_loss: 1.9265 - val_accuracy: 0.5293\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.9358 - accuracy: 0.5065 - val_loss: 1.6061 - val_accuracy: 0.7000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6233 - accuracy: 0.6814 - val_loss: 1.3354 - val_accuracy: 0.7726\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.3588 - accuracy: 0.7555 - val_loss: 1.1109 - val_accuracy: 0.8133\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.1388 - accuracy: 0.7966 - val_loss: 0.9338 - val_accuracy: 0.8326\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.9652 - accuracy: 0.8173 - val_loss: 0.7981 - val_accuracy: 0.8464\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.8318 - accuracy: 0.8288 - val_loss: 0.6942 - val_accuracy: 0.8534\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7294 - accuracy: 0.8359 - val_loss: 0.6139 - val_accuracy: 0.8596\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6495 - accuracy: 0.8443 - val_loss: 0.5514 - val_accuracy: 0.8658\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5869 - accuracy: 0.8518 - val_loss: 0.5028 - val_accuracy: 0.8737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcpSk20qjv-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "8dba2af2-39ec-43f2-c71a-7602e8f4b84e"
      },
      "source": [
        "df = pd.DataFrame(data, columns = ['neurons', 'activation', 'batch_size','val_accuracy'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>neurons</th>\n",
              "      <th>activation</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>800</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5000</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>800</td>\n",
              "      <td>linear</td>\n",
              "      <td>128</td>\n",
              "      <td>0.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>800</td>\n",
              "      <td>relu</td>\n",
              "      <td>1</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>800</td>\n",
              "      <td>relu</td>\n",
              "      <td>10</td>\n",
              "      <td>0.979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>800</td>\n",
              "      <td>relu</td>\n",
              "      <td>100</td>\n",
              "      <td>0.980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>800</td>\n",
              "      <td>relu</td>\n",
              "      <td>60000</td>\n",
              "      <td>0.872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   neurons activation  batch_size  val_accuracy\n",
              "0      800       relu         128         0.981\n",
              "1       10       relu         128         0.932\n",
              "2      100       relu         128         0.976\n",
              "3     5000       relu         128         0.983\n",
              "4      800     linear         128         0.917\n",
              "5      800       relu           1         0.973\n",
              "6      800       relu          10         0.979\n",
              "7      800       relu         100         0.980\n",
              "8      800       relu       60000         0.872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XipGvuLpmnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Выводы: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh-pPGWkOp1o",
        "colab_type": "text"
      },
      "source": [
        "# Pro\n",
        "\n",
        "Распознайте рукописную цифру, созданную вами с помощью графического редактора (например, Paint).\n",
        "\n",
        "Последовательность шагов:\n",
        "\n",
        "В графическом редакторе рисуем произвольную цифру.\n",
        "Сохраняем документ и загружаем его в Collaboratory.\n",
        "С помощью функции image.load_img(path, target_size=(28, 28), color_mode = ‘grayscale’) загружаем картинку в переменную.\n",
        "С помощью функции image.img_to_array(img) преобразуем изображение в numpy-массив.\n",
        "Выполняем инверсию цветов, нормирование и решейп массива.\n",
        "\n",
        "\n",
        "Выполняем распознавание собственной рукописной цифры.\n",
        "\n",
        "Реализуйте тот же алгоритм для написанной на листе от руки цифры (для этого необходимо сделать дополнительную предобработку изображения).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEPPQQuvpSXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "041e2700-75fd-48e3-a4ca-1b3038250677"
      },
      "source": [
        "# Загружаем картинку сделанную в графическом редакторе\n",
        "example = image.load_img('/content/4_.png', target_size=(28, 28), color_mode = 'grayscale') \n",
        "\n",
        "# Нарисуем картинку\n",
        "plt.imshow(example.convert('RGBA')) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuvzJ9fopXg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Нормализуем данные\n",
        "example = image.img_to_array(example) # преобразуем изображение в numpy-массив\n",
        "example = example.reshape(1,784)\n",
        "example = example.astype('float32')\n",
        "example = 1- example/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cu37Ii1pXn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6513de90-5207-4e96-e524-7c87afadf963"
      },
      "source": [
        "#Распознаём наш пример\n",
        "pred_example = model_sample.predict(example)\n",
        "print(pred_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.7782714e-09 8.3022212e-07 1.2345195e-06 1.3279305e-04 9.8141897e-01\n",
            "  1.3637092e-04 3.4444900e-05 4.0913419e-05 8.5871518e-03 9.6471692e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ntvxVzpXtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52f8fbbc-8a70-45cd-bc60-7e17fe04e0fd"
      },
      "source": [
        "# Получаем индекс самого большого элемента (это итоговая цифра, которую распознала сеть)\n",
        "pred = np.argmax(pred_example)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hQB5V4cst1PH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "920ddbb4-bf4e-4c02-864f-fc840e25c9b9"
      },
      "source": [
        "# Загружаем картинку нарисованную на листе бумаги\n",
        "example = image.load_img('/content/6_.png', target_size=(28, 28), color_mode = 'grayscale') \n",
        "\n",
        "# Нарисуем картинку\n",
        "plt.imshow(example.convert('RGBA')) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EOxaMJGuumx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "e71061e8-7b95-49b9-992b-e82579b1ca9c"
      },
      "source": [
        "example = image.img_to_array(example)\n",
        "example = example.reshape(1,784)\n",
        "print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[213. 210. 208. 205. 207. 208. 211. 209. 209. 208. 208. 206. 209. 211.\n",
            "  209. 210. 210. 211. 208. 210. 212. 212. 213. 213. 210. 216. 215. 214.\n",
            "  206. 209. 205. 209. 207. 209. 212. 208. 208. 211. 206. 208. 210. 209.\n",
            "  209. 212. 209. 215. 211. 209. 212. 213. 213. 211. 213. 212. 211. 211.\n",
            "  209. 214. 205. 208. 206. 214. 210. 209. 207. 209. 209. 210. 213. 211.\n",
            "  207. 214. 211. 212. 207. 212. 212. 207. 210. 215. 212. 213. 217. 214.\n",
            "  208. 209. 212. 209. 206. 208. 211. 210. 211. 209. 212. 208. 209. 207.\n",
            "  208. 211. 211. 211. 211. 211. 211. 211. 211. 211. 215. 210. 210. 212.\n",
            "  211. 207. 208. 209. 204. 206. 206. 208. 211. 208. 211. 208. 207. 207.\n",
            "  216. 211. 214. 217. 214. 210. 211. 212. 211. 211. 213. 210. 212. 212.\n",
            "  206. 208. 211. 206. 208. 208. 208. 209. 209. 207. 209. 209. 209. 209.\n",
            "  208. 122.  83.  55.  59. 188. 211. 213. 210. 210. 209. 211. 211. 211.\n",
            "  210. 208. 208. 206. 205. 208. 208. 213. 211. 204. 210. 208. 207. 217.\n",
            "   92.  68. 209. 209.  55. 107. 210. 210. 212. 212. 211. 211. 211. 211.\n",
            "  207. 208. 208. 206. 208. 208. 208. 209. 207. 208. 207. 205. 214.  75.\n",
            "   62. 210. 208. 202.  51. 209. 207. 211. 212. 210. 212. 210. 210. 213.\n",
            "  205. 208. 208. 207. 208. 204. 206. 206. 209. 210. 208. 207.  66.  56.\n",
            "  208. 207. 214. 212. 211. 211. 209. 212. 211. 212. 212. 210. 209. 210.\n",
            "  203. 205. 208. 204. 205. 207. 207. 209. 208. 208. 213. 117.  53. 205.\n",
            "  210. 210. 211. 211. 212. 212. 209. 210. 212. 210. 210. 211. 211. 211.\n",
            "  208. 207. 209. 208. 211. 202. 208. 208. 208. 209. 199.  38. 219. 211.\n",
            "  209. 208. 210. 211. 212. 211. 211. 212. 212. 210. 213. 210. 211. 212.\n",
            "  206. 209. 207. 209. 208. 206. 209. 211. 210. 211.  56. 105. 209. 209.\n",
            "  209. 209. 208. 211. 210. 211. 211. 212. 209. 209. 209. 210. 210. 210.\n",
            "  208. 206. 209. 208. 209. 209. 212. 213. 210. 199.  46. 226. 209. 208.\n",
            "  209. 209. 210. 212. 210. 210. 210. 210. 212. 212. 213. 212. 211. 211.\n",
            "  209. 208. 209. 208. 211. 211. 209. 207. 209.  47.  61. 212. 207. 207.\n",
            "  187.  77. 172. 210. 212. 210. 211. 210. 212. 212. 209. 210. 211. 211.\n",
            "  207. 209. 211. 209. 208. 211. 210. 208. 202.  87. 148. 214.  95.  34.\n",
            "   56.  56.  72. 135. 209. 212. 211. 212. 210. 210. 209. 213. 210. 213.\n",
            "  211. 208. 210. 208. 210. 210. 209. 208. 198.  49. 142.  60.  59. 216.\n",
            "  211. 208.  63.  68. 186. 211. 210. 212. 211. 211. 212. 211. 213. 210.\n",
            "  208. 208. 208. 209. 209. 209. 209. 208. 132.  51.  45.  65. 217. 211.\n",
            "  211. 212.  52.  44. 209. 211. 212. 211. 211. 211. 213. 211. 210. 210.\n",
            "  206. 209. 211. 210. 208. 208. 209. 208.  79.  55. 208. 215. 211. 212.\n",
            "  212. 215.  70.  73. 210. 211. 210. 211. 212. 212. 211. 211. 212. 210.\n",
            "  208. 209. 205. 205. 210. 203. 210. 209. 129.  62. 211. 209. 214. 214.\n",
            "  212.  56.  76. 222. 211. 211. 210. 211. 211. 211. 209. 211. 210. 211.\n",
            "  208. 206. 208. 210. 211. 212. 209. 212. 211.  62. 220. 210. 209. 215.\n",
            "   37.  57. 221. 212. 213. 213. 212. 213. 212. 212. 211. 210. 210. 209.\n",
            "  207. 207. 210. 208. 210. 213. 209. 209. 209. 158.  54.  69.  73.  51.\n",
            "  185. 213. 212. 212. 211. 211. 211. 212. 212. 212. 212. 210. 209. 211.\n",
            "  207. 209. 210. 210. 210. 210. 208. 209. 211. 210. 215. 170.  59. 219.\n",
            "  207. 210. 209. 211. 212. 211. 211. 208. 211. 211. 211. 212. 211. 210.\n",
            "  207. 209. 210. 211. 210. 210. 210. 211. 209. 210. 211. 208. 212. 212.\n",
            "  214. 211. 209. 211. 210. 211. 211. 210. 211. 211. 209. 212. 213. 210.\n",
            "  209. 207. 210. 209. 209. 209. 209. 209. 209. 210. 210. 210. 210. 210.\n",
            "  210. 211. 211. 211. 211. 211. 211. 210. 211. 213. 208. 209. 210. 210.\n",
            "  208. 208. 208. 210. 208. 206. 207. 208. 208. 209. 209. 208. 209. 209.\n",
            "  209. 211. 210. 210. 211. 210. 210. 210. 212. 211. 208. 210. 209. 209.\n",
            "  207. 209. 207. 207. 207. 207. 208. 209. 210. 208. 207. 209. 209. 209.\n",
            "  211. 211. 210. 211. 210. 209. 208. 209. 208. 208. 208. 207. 209. 209.\n",
            "  206. 207. 207. 207. 207. 208. 208. 208. 205. 208. 208. 208. 208. 209.\n",
            "  209. 209. 207. 210. 211. 210. 208. 208. 207. 208. 208. 208. 208. 208.\n",
            "  206. 204. 208. 211. 207. 207. 207. 207. 206. 208. 208. 209. 208. 208.\n",
            "  208. 208. 208. 207. 208. 208. 207. 207. 209. 209. 209. 210. 209. 208.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTLqUiUcuZRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a260542-f2c3-40d7-c700-7ca122d991e2"
      },
      "source": [
        "example = np.where(example > 100, 255, example) # По условию меняем значения в масиве\n",
        "example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255.,  83.,  55.,  59., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255.,  92.,  68., 255., 255.,  55.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "         75.,  62., 255., 255., 255.,  51., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255.,  66.,  56., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "         53., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255.,  38., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,  56.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255.,  46., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,  47.,\n",
              "         61., 255., 255., 255., 255.,  77., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255.,  87., 255., 255.,  95.,  34.,  56.,\n",
              "         56.,  72., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "         49., 255.,  60.,  59., 255., 255., 255.,  63.,  68., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255.,  51.,  45.,  65., 255., 255.,\n",
              "        255., 255.,  52.,  44., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "         79.,  55., 255., 255., 255., 255., 255., 255.,  70.,  73., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255.,  62., 255., 255., 255.,\n",
              "        255., 255.,  56.,  76., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255.,  62., 255., 255., 255., 255.,  37.,  57., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255.,  54.,  69.,\n",
              "         73.,  51., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255.,  59., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
              "        255., 255., 255.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpNgJoCd39e0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "883a6f67-a701-4e62-a2df-7823e205b5e2"
      },
      "source": [
        "plt.imshow(Image.fromarray(example.reshape(28,28)).convert('RGBA')) # Нарисуем картинку после обработки\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wKX4mT5jt1PR",
        "colab": {}
      },
      "source": [
        "# Нормализуем данные\n",
        "example = example.astype('float32')\n",
        "example = 1 - example/255 # Делаем инверсию, чтобы было не черное на белом, а белая цифра на черном фоне"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I4nRzb8St1PV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32fae50c-2061-40cf-824c-6b108f9efd70"
      },
      "source": [
        "#Распознаём наш пример\n",
        "pred_example = model_sample.predict(example)\n",
        "print(pred_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.7810914e-04 2.7576803e-05 4.5037396e-02 7.6707496e-05 5.4097213e-03\n",
            "  3.4460474e-03 9.4286102e-01 1.7722134e-05 2.5518632e-03 1.9384034e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OoSWk72nt1PZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d17af8d9-23a0-4746-f4cc-952851606b08"
      },
      "source": [
        "# Получаем индекс самого большого элемента (это итоговая цифра, которую распознала сеть)\n",
        "pred = np.argmax(pred_example)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kArUq9ww32wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}